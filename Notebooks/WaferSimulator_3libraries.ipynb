{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import matplotlib as mpl\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "mpl.rcParams['mathtext.default'] = 'regular'\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wafer simulation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Func 1: Create dataset with coordinates\n",
    "- Functionality: make a rectangular grid, create dataset with coordinates\n",
    "- Inputs: shape (circle or square), diameter (use motor units like mm), resolution (beam size, match units of diameter)\n",
    "- Returns: dataset with coordinates, attributes of shape, diameter, resolution (to be used in other functions)\n",
    "\n",
    "Func 2: Calculate elemental composition at each point\n",
    "- Functionality: Set elemental composition at discrete points (manually entered or calculated at distance from center/edge)\n",
    "- Inputs: 1) dataset (shape, diameter, resolution are attributes than can be accessed!) 2) num_positions (N) 3) elements (E) 4) element_comps (E, N) 5) method can be manual or equidistant, 6) smoothing factor=1.0\n",
    "    - How do we have method specific arguments? e.g., if manual must provide ((xs,ys), N), if equidistant must provide % from center to edge \n",
    "    - Add data variable for elemental_weights with 0s\n",
    "        - if shape == circle, set outside circle to NaNs\n",
    "- Returns: dataset with elements and element_weights, with NaNs for weights outside shape (if shape == circle)\n",
    "    - Additional attributes are: elements, element_comps + positions, smoothing factor\n",
    "\n",
    "Func 3: Interpolate phase_weights, I(Q) at each point\n",
    "- Functionality:\n",
    "- Inputs: dataset, phase diagram dataset (needed for interpolation), \n",
    "- Returns: dataset (ground_truth) with phases, phase_weights, I(Q) at each point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_root() -> Path:\n",
    "    # 1) If running inside a Git clone, ask Git\n",
    "    try:\n",
    "        root = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--show-toplevel\"], text=True\n",
    "        ).strip()\n",
    "        return Path(root)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) Optional: allow an override via env var\n",
    "    if os.getenv(\"PROJECT_ROOT\"):\n",
    "        return Path(os.environ[\"PROJECT_ROOT\"]).expanduser().resolve()\n",
    "    # 3) Fallback: current working directory\n",
    "    print('Root directory not found, using current working directory')\n",
    "    return Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_coords(shape, diameter, resolution) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : string\n",
    "        circle or square\n",
    "\n",
    "    diameter : float\n",
    "        diameter in motor coord units (typically mm)\n",
    "\n",
    "    resolution: float\n",
    "        beam size (same units as diameter)\n",
    "    \"\"\"\n",
    "    # Create coordinate grid\n",
    "    pts_per_side = int(np.ceil(diameter / resolution)) # number of points per side\n",
    "    x = np.linspace(-diameter//2, diameter//2, pts_per_side)\n",
    "    y = np.linspace(-diameter//2, diameter//2, pts_per_side)\n",
    "    xv, yv = np.meshgrid(x, y) # create grid coordinates from 1D x,y arrays. xv, yv are 2D arays for each point on the grid\n",
    "    coords = np.column_stack([xv.ravel(), yv.ravel()])\n",
    "\n",
    "    if shape == 'circle':\n",
    "        x_coords = coords[:,0]\n",
    "        y_coords = coords[:,1]\n",
    "        center = (0,0)\n",
    "        distances = np.sqrt((x_coords - center[0])**2 + (y_coords - center[1])**2)\n",
    "        mask = distances >= (diameter / 2)\n",
    "        points_inside = len([i for i in mask if not i])\n",
    "        #valid_coords = np.array([coords[i] for i in range(coords.shape[0]) if mask[i] == False])\n",
    "        #coords_mask = np.reshape(mask, (len(x),len(x),1))\n",
    "\n",
    "    #else:\n",
    "    #    valid_coords = coords\n",
    "\n",
    "    # create dataset functionality below here\n",
    "    ds = xr.Dataset(\n",
    "        coords={\n",
    "            \"x\": (\"points\", coords[:,0]),\n",
    "            \"y\": (\"points\",coords[:,1]),\n",
    "            \"xy\": ((\"points\", \"tuple_index\"), coords)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds.attrs['description'] = f\"Simulated dataset with shape = {shape}, diameter = {diameter}, resolution = {resolution}, and points inside = {points_inside}.\"\n",
    "    ds.attrs['shape'] = shape\n",
    "    ds.attrs['shape_center'] = (0,0)\n",
    "    ds.attrs['shape_diameter'] = diameter\n",
    "    ds.attrs['resolution'] = resolution\n",
    "    ds.attrs['points_inside'] = points_inside\n",
    "    #ds.attrs['valid_coords'] = valid_coords\n",
    "\n",
    "    #if shape == 'circle':\n",
    "    #    ds.attrs['mask_2D'] =  coords_mask\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def calc_elemental_comps(dataset: xr.Dataset, num_compositions: int, elements_list, discrete_compositions, positions='calculated',\n",
    "                         calc_dist_scale = 100, deg_rotation=-90,\n",
    "                         discrete_comp_coords = None,\n",
    "                         find_on_grid=True,\n",
    "                         smoothing_factor=1.0) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Xarray dataset created from create_dataset_with_coords()\n",
    "\n",
    "    num_compositions : int\n",
    "        Number of discrete compositions to be set\n",
    "\n",
    "    elements_list : list of strings\n",
    "        element names (e.g., ['Al', 'Li', 'Fe']) matching order of discrete_compositions\n",
    "\n",
    "    discrete_compositions : array of arrays, where shape = (num_compositions, len(elements_list))\n",
    "        Elements of arrays are floats between 0 and 1. Each sub-array must sum to 1\n",
    "\n",
    "    positions : string\n",
    "        Options are 'calculated' or 'manual'    ### IS THIS THE BEST WAY TO HANDLE THIS? HOW CAN WE SEPARATE EQUIDISTANT CALC + DIST_FROM_CENTER_TO_EDGE_PERCENT AND MANUAL INPUTS?\n",
    "            If 'calculated', calculates equidistant points around a shape and scales positions from center to edge based on radius - these points are the coordinate positions for discrete_compositions\n",
    "                Required inputs: equidistant_scale, deg_rotation, find_on_grid\n",
    "            If 'manual', the positions are manually defined\n",
    "                Required inputs: discrete_comp_coords, find_on_grid\n",
    "\n",
    "    calc_dist_scale : float\n",
    "        Percent distance from shape center to edge for calculating equidistant points where 0 = at center and 100 = at edge\n",
    "\n",
    "    deg_rotation : float\n",
    "        Rotating calculated points around a circle - defaulted to -90\n",
    "\n",
    "    find_on_grid : bool\n",
    "        If True, finds coordinates nearest calculated or inputted positions. If False, uses exact positions calculated or inputted.\n",
    "\n",
    "    discrete_comp_coords : array of arrays where shape = (num_compositions, 2).\n",
    "        Elements (floats) are x,y coordinate pairs corresponding to discrete_compositions\n",
    "\n",
    "    smoothing_factor : float\n",
    "        Controls gaussian smoothing between discrete compositions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Xarray dataset with elements and element_weights\n",
    "    \"\"\"\n",
    "    # Lots of error checking is needed - this is not exhaustive but should catch most major errors\n",
    "    if num_compositions != discrete_compositions.shape[0]:\n",
    "        raise ValueError(f\"num_compositions ({num_compositions}) must be an integer matching discrete_compositions.shape[0] ({discrete_compositions.shape[0]})\")\n",
    "\n",
    "    if discrete_compositions.shape != (num_compositions, len(elements_list)):\n",
    "        raise ValueError(f\"discrete_compositions must have shape of (num_compositions, len(elements_list)) - currently {discrete_compositions.shape}\")\n",
    "\n",
    "    if positions == 'calculated':\n",
    "        if not (0 <= calc_dist_scale >= 100):\n",
    "            raise ValueError(f\"Positions are 'calculated' so calc_dist_scale must be between 0 and 100 % (currently = {calc_dist_scale})\")\n",
    "\n",
    "    if discrete_comp_coords is not None:\n",
    "        if positions == 'calculated':\n",
    "            raise ValueError(f\"discrete_comp_coords is not None, but positions are 'calculated' - must have 'manual' positions or set discrete_comp_coords to None\")\n",
    "        elif positions =='manual':\n",
    "            if discrete_comp_coords.shape != (num_compositions, 2):\n",
    "                raise ValueError(f\"discrete_comp_coords must have shape (num_compositions, 2) - current shape is {discrete_comp_coords.shape}\")\n",
    "\n",
    "    for num, sublist in enumerate(discrete_compositions):\n",
    "        if np.sum(sublist) != 1:\n",
    "            raise ValueError(f\"Sublist {num} in discrete_compositions does not sum to 1. Ensure sublist elements sum to 1 and try again.\\nSublist = {sublist}. Sum = {np.sum(sublist)}\")\n",
    "\n",
    "    # Composition center calculations\n",
    "\n",
    "    if positions == 'calculated':\n",
    "        if dataset.attrs['shape'] == 'circle':\n",
    "            radius = dataset.attrs['shape_diameter'] / 2 # this is true for both square and circle\n",
    "            center = dataset.attrs['shape_center'] # assuming always centering at origin\n",
    "\n",
    "            # Calculate angles in radians given a number of compositions - endpoint = False ensures 0=360\n",
    "            # deg_rotation allows for rotating the points to a desired position (e.g., set 1st point at bottom of shape)\n",
    "            anglesrad = np.linspace(0, 2 * np.pi, num_compositions, endpoint=False)\n",
    "            anglesdeg = [np.degrees(angle) + deg_rotation for angle in anglesrad]\n",
    "            angles = np.radians(anglesdeg)\n",
    "\n",
    "            # Calculate the x and y coordinates\n",
    "            x_positions = center[0] + (radius * calc_dist_scale *0.01) * np.cos(angles)\n",
    "            y_positions = center[1] + (radius * calc_dist_scale * 0.01)  * np.sin(angles)\n",
    "\n",
    "        elif dataset.attrs['shape'] == 'square':\n",
    "            raise ValueError(\"I still have to write equidistant point calculations for a square - come back later\")\n",
    "\n",
    "    elif positions == 'manual':\n",
    "        # already checked that discrete_comp_coords is defined and the right shape\n",
    "        x_positions = discrete_comp_coords[:,0]\n",
    "        y_positions = discrete_comp_coords[:,1]\n",
    "\n",
    "    #\n",
    "    # Find on grid vs. using exact coords\n",
    "    #\n",
    "\n",
    "    # Find nearest coordinate for the x and y coords\n",
    "    if find_on_grid is True:\n",
    "        comp_points = np.zeros((num_compositions,2))\n",
    "        for i, (x_pos, y_pos) in enumerate(zip(x_positions,y_positions)):\n",
    "            distances = np.sqrt((dataset.x.values - x_pos)**2 + (dataset.y.values - y_pos)**2)\n",
    "            nearest_index = int(distances.argmin())\n",
    "            nearest_point = dataset.xy.values[nearest_index]\n",
    "            comp_points[i] = nearest_point\n",
    "    else:\n",
    "        comp_points = np.column_stack((x_positions,y_positions)) # use calculated or inputted positions directly\n",
    "\n",
    "    #\n",
    "    # Calculate composition weights\n",
    "    #\n",
    "\n",
    "    x_coords = dataset['x'].values\n",
    "    y_coords = dataset['y'].values\n",
    "    coords = np.column_stack((x_coords, y_coords))\n",
    "\n",
    "    # Number of points and elements\n",
    "    num_points = coords.shape[0]\n",
    "    num_elements = len(elements_list)\n",
    "\n",
    "    # Initialize weights array\n",
    "    weights = np.zeros((num_points, num_elements))\n",
    "\n",
    "    # Track points that match composition centers\n",
    "    fixed_points = np.zeros(num_points, dtype=bool)\n",
    "\n",
    "    # Identify matching points\n",
    "    for i, (x_center, y_center) in enumerate(comp_points):\n",
    "        matching_points = (coords[:, 0] == x_center) & (coords[:, 1] == y_center)\n",
    "        weights[matching_points] = discrete_compositions[i]\n",
    "        fixed_points |= matching_points\n",
    "\n",
    "    # Calculate weights for non-fixed points\n",
    "    for i, (x_center, y_center) in enumerate(comp_points):\n",
    "        # Calculate distances\n",
    "        distances = np.sqrt((coords[:, 0] - x_center) ** 2 + (coords[:, 1] - y_center) ** 2)\n",
    "\n",
    "        # Apply Gaussian smoothing and reshape\n",
    "        exp_distances = np.exp(-distances / smoothing_factor)  # shape = num_points\n",
    "        exp_distances_reshaped = exp_distances.reshape(-1, 1)  # shape = num_points, 1\n",
    "\n",
    "        # Multiply distances with composition_at_centers[i] for non-fixed points\n",
    "        weighted_composition_at_center = exp_distances_reshaped * discrete_compositions[i]  # shape = num_points, num_elements\n",
    "\n",
    "        # Add weighted composition to weights for non-fixed points\n",
    "        weights[~fixed_points] += weighted_composition_at_center[~fixed_points]\n",
    "\n",
    "    # Normalize weights to sum to 1 for each point\n",
    "    weights_sum = weights.sum(axis=1)  # shape = num_points\n",
    "    weights_normalized = weights / weights_sum[:, np.newaxis]\n",
    "    weights = weights_normalized\n",
    "\n",
    "    # Precision issue with weights is annoying for interpolation, so if sum of weights for a row is within 1e-8, divide difference by N elements and subtract/add\n",
    "    weight_tolerance = 1e-20\n",
    "\n",
    "    for index, weight in enumerate(weights):\n",
    "        weight_sum = np.sum(weight)\n",
    "        if not np.isclose(weight_sum, 1.0, atol=weight_tolerance):\n",
    "            weight_diff = 1 - weight_sum\n",
    "            if np.abs(weight_diff) < weight_tolerance:\n",
    "                if weight_diff > 0:  # weight sum less than 1\n",
    "                    weights[index, 0] += weight_diff\n",
    "                else:  # weight sum greater than 1\n",
    "                    weights[index, 0] -= weight_diff\n",
    "            else:\n",
    "                raise ValueError(f'Summed weight for index {index} outside tolerance for adjustment - inspect calculation of weights')\n",
    "\n",
    "    # Check if all rows sum to 1 within tolerance\n",
    "    counter = 0\n",
    "    for index, weight in enumerate(weights):\n",
    "        if not np.isclose(np.sum(weight), 1.0, atol=weight_tolerance):\n",
    "            print(index, np.sum(weight), weight)\n",
    "            counter += 1\n",
    "    if counter != 0:\n",
    "        raise ValueError(f\"Number of weights (rows) not summing to 1: {counter}\")\n",
    "\n",
    "    # set weights to NaN where points are outside\n",
    "    if dataset.attrs['shape'] == 'circle':\n",
    "        diameter = dataset.attrs['shape_diameter']\n",
    "        center = dataset.attrs['shape_center'] # assuming always centering at origin\n",
    "\n",
    "        distances = np.sqrt((x_coords - center[0])**2 + (y_coords - center[1])**2)\n",
    "        mask = distances >= (diameter / 2)\n",
    "\n",
    "        weights[mask, :] = np.nan\n",
    "\n",
    "    # Assign coordinates and weights to the dataset\n",
    "    repeated_elements = np.tile(elements_list, (num_points, 1))\n",
    "\n",
    "    dataset = dataset.assign_coords(\n",
    "        elements=((\"points\", \"elements\"), repeated_elements),\n",
    "        element_weights=((\"points\", \"weights\"), weights)\n",
    "    )\n",
    "    discrete_compositions_str = [str(comp) for comp in discrete_compositions]\n",
    "    comp_points_str = [str(pos) for pos in comp_points]\n",
    "    dataset.attrs['composition_centers (coords)'] = comp_points_str\n",
    "    dataset.attrs['composition_centers (weights)'] = discrete_compositions_str\n",
    "    dataset.attrs['smoothing_factor'] = smoothing_factor\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def interpolate_phase_weights(element_weights_wafer, element_weights_PD, phase_weights_PD):\n",
    "    # Initialize phase_weights_wafer with NaNs\n",
    "    phase_weights_wafer = np.full((element_weights_wafer.shape[0], phase_weights_PD.shape[1]), np.nan)\n",
    "\n",
    "    # Perform the interpolation for each dimension of the phase weights\n",
    "    for i in trange(phase_weights_PD.shape[1], desc='Phase weights interpolation loop'):\n",
    "        rbf_interpolator = RBFInterpolator(element_weights_PD, phase_weights_PD[:, i], kernel='linear')\n",
    "        interpolated_values = rbf_interpolator(element_weights_wafer)\n",
    "\n",
    "        # Ensure non-negative values\n",
    "        interpolated_values = np.clip(interpolated_values, 0, np.inf)\n",
    "\n",
    "        phase_weights_wafer[:, i] = interpolated_values\n",
    "\n",
    "    # Normalize the rows to sum to 1.0\n",
    "    row_sums = np.sum(phase_weights_wafer, axis=1, keepdims=True)\n",
    "    # Avoid division by zero by setting row_sums to 1 where it's 0\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    phase_weights_wafer = phase_weights_wafer / row_sums\n",
    "\n",
    "    return phase_weights_wafer\n",
    "\n",
    "def interpolate_iq_wafer(phase_weights_wafer, phase_weights_PD, iq_PD_ionly, epsilon=1e-10):\n",
    "    # Initialize iq_wafer_ionly with NaNs\n",
    "    iq_wafer_ionly = np.full((phase_weights_wafer.shape[0], iq_PD_ionly.shape[1]), np.nan)\n",
    "\n",
    "    # Perform the interpolation for each dimension of the phase weights\n",
    "    for i in trange(iq_PD_ionly.shape[1], desc='Intensity interpolation loop'):\n",
    "        try:\n",
    "            # Add a small perturbation to phase_weights_PD to avoid singular matrix issues\n",
    "            phase_weights_PD_perturbed = phase_weights_PD + epsilon * np.random.randn(*phase_weights_PD.shape)\n",
    "\n",
    "            rbf_interpolator = RBFInterpolator(phase_weights_PD_perturbed, iq_PD_ionly[:, i], kernel='linear')\n",
    "            interpolated_values = rbf_interpolator(phase_weights_wafer)\n",
    "\n",
    "            # Ensure non-negative values\n",
    "            #interpolated_values = np.clip(interpolated_values, 0, np.inf)\n",
    "\n",
    "            iq_wafer_ionly[:, i] = interpolated_values\n",
    "        except Exception as e:\n",
    "            print(f\"Error interpolating dimension {i}: {e}\")\n",
    "\n",
    "    return iq_wafer_ionly\n",
    "\n",
    "def interpolate_and_addtods(dataset, dataset_DRNets, noise_percentage=0.01):\n",
    "\n",
    "    def add_noise_by_percentage(signal, noise_percentage):\n",
    "        \"\"\"\n",
    "        Add random noise to a NumPy array based on a percentage of the maximum signal value.\n",
    "\n",
    "        Parameters:\n",
    "            signal (numpy.ndarray): The original signal array.\n",
    "            noise_percentage (float): Percentage of the maximum signal value to use as noise.\n",
    "\n",
    "        Returns:\n",
    "            noisy_signal (numpy.ndarray): The signal array with added noise.\n",
    "        \"\"\"\n",
    "        # Find the maximum value in the signal\n",
    "        max_value = np.max(np.abs(signal))\n",
    "\n",
    "        # Calculate the noise standard deviation as a percentage of the max signal value\n",
    "        noise_std = (noise_percentage / 100) * max_value\n",
    "\n",
    "        # Generate random Gaussian noise with zero mean and calculated standard deviation\n",
    "        noise = np.random.normal(0, noise_std, signal.shape)\n",
    "\n",
    "        # Add the noise to the original signal\n",
    "        noisy_signal = signal + noise\n",
    "\n",
    "        return noisy_signal\n",
    "\n",
    "    # Get arrays from ds and ds_DRNets\n",
    "    element_weights_wafer = dataset['element_weights'].values  # shape (709, 3)\n",
    "    element_weights_PD = dataset_DRNets['element_weights'].values  # shape (231, 3)\n",
    "\n",
    "    phase_weights_PD = dataset_DRNets['phase_weights'].values  # shape (231, 6)\n",
    "\n",
    "    iq_PD = dataset_DRNets['iq'].values\n",
    "    iq_PD_ionly = iq_PD[:, 1, :] # only grabbing the intensity array, shape (231, 650)\n",
    "    q_points = iq_PD[0][0]\n",
    "\n",
    "    phase_weights_wafer = interpolate_phase_weights(element_weights_wafer, element_weights_PD, phase_weights_PD)\n",
    "    iq_wafer_ionly = interpolate_iq_wafer(phase_weights_wafer, phase_weights_PD, iq_PD_ionly)\n",
    "\n",
    "    for num, pattern in enumerate(iq_wafer_ionly):\n",
    "        if not np.isnan(pattern).any():\n",
    "            noisy_pattern = add_noise_by_percentage(signal=pattern, noise_percentage=noise_percentage)\n",
    "            iq_wafer_ionly[num] = noisy_pattern  # Explicitly assign back the noisy pattern\n",
    "\n",
    "    iq_wafer = np.empty((iq_wafer_ionly.shape[0], 2, len(q_points)))\n",
    "\n",
    "    # Fill the first slice of the second axis with q_points\n",
    "    iq_wafer[:, 0, :] = q_points\n",
    "\n",
    "    # Fill the second slice of the second axis with iq_wafer_ionly\n",
    "    iq_wafer[:, 1, :] = iq_wafer_ionly\n",
    "\n",
    "    # Get phase names, tile to dataset shape\n",
    "    phase_names = dataset_DRNets['phase_names'][0].values\n",
    "    repeated_names = np.tile(phase_names, (phase_weights_wafer.shape[0], 1))\n",
    "\n",
    "    dataset = dataset.assign_coords(\n",
    "        phase_names=((\"points\", \"names\"), repeated_names),\n",
    "    )\n",
    "\n",
    "    # Add phase_weights, I(Q) to dataset\n",
    "    dataset['phase_weights'] = (('points', 'phase_weights'), phase_weights_wafer)\n",
    "    dataset['iq'] = (('points', 'tuple_index', 'q_points'), iq_wafer)\n",
    "\n",
    "    # Add Q array as an attribute (does not need to be stored with intensity)\n",
    "    dataset.attrs['Q'] = q_points\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def reshape_ds(dataset):\n",
    "    pts_per_side = int(np.ceil(dataset.attrs['shape_diameter'] / dataset.attrs['resolution'])) # number of points per side\n",
    "    x = np.linspace(-dataset.attrs['shape_diameter']//2, dataset.attrs['shape_diameter']//2, pts_per_side)\n",
    "    y = np.linspace(-dataset.attrs['shape_diameter']//2, dataset.attrs['shape_diameter']//2, pts_per_side)\n",
    "    N = len(x)\n",
    "\n",
    "    arr_ew = np.reshape(dataset.element_weights.data, (N,N,dataset.element_weights.data.shape[1]))\n",
    "    da_ew = xr.DataArray(arr_ew, dims=[\"y\", \"x\", \"element_weight\"], coords=dict(y=y, x=x, element_weight=(\"element_weight\", [str(i) for i in dataset.elements.data[0]])))\n",
    "\n",
    "    arr_pw = np.reshape(dataset.phase_weights.data, (N,N,dataset.phase_weights.data.shape[1]))\n",
    "    da_phase = xr.DataArray(arr_pw, dims=[\"y\", \"x\", \"phase_weight\"], coords=dict(y=y, x=x, phase_weight=(\"phase_weight\", [str(i) for i in dataset.phase_names.data[0]])))\n",
    "\n",
    "    arr_iq = np.reshape(dataset.iq.data[:,1,:], (N,N,dataset.iq.data.shape[2]))\n",
    "    da_iq = xr.DataArray(arr_iq, dims=[\"y\", \"x\", \"intensity\"], coords=dict(y=y, x=x))\n",
    "\n",
    "    reshaped_ds = xr.Dataset({\n",
    "        'element_weights': da_ew,\n",
    "        'phase_weights': da_phase,\n",
    "        'iq': da_iq\n",
    "        })\n",
    "\n",
    "    # Copy in some existing attributes, add some new ones\n",
    "    reshaped_ds.attrs['description'] = dataset.attrs['description']\n",
    "    reshaped_ds.attrs['elements'] = [str(i) for i in dataset.elements.data[0]]\n",
    "    reshaped_ds.attrs['phases'] = [str(i) for i in dataset.phase_names.data[0]]\n",
    "    reshaped_ds.attrs['shape'] = dataset.attrs['shape']\n",
    "    reshaped_ds.attrs['shape_center'] = dataset.attrs['shape_center']\n",
    "    reshaped_ds.attrs['shape_width'] = dataset.attrs['shape_diameter']\n",
    "    reshaped_ds.attrs['resolution'] = dataset.attrs['resolution']\n",
    "    reshaped_ds.attrs['points_inside'] = dataset.attrs['points_inside']\n",
    "    reshaped_ds.attrs['composition_centers (coords)'] = dataset.attrs['composition_centers (coords)']\n",
    "    reshaped_ds.attrs['composition_centers (weights)'] = dataset.attrs['composition_centers (weights)']\n",
    "    reshaped_ds.attrs['smoothing_factor'] = dataset.attrs['smoothing_factor']\n",
    "    reshaped_ds.attrs['Q'] = dataset.attrs['Q']\n",
    "\n",
    "    return reshaped_ds\n",
    "\n",
    "def reshape_ds_simple(dataset):\n",
    "    pts_per_side = int(np.ceil(dataset.attrs['shape_diameter'] / dataset.attrs['resolution'])) # number of points per side\n",
    "    x = np.linspace(-dataset.attrs['shape_diameter']//2, dataset.attrs['shape_diameter']//2, pts_per_side)\n",
    "    y = np.linspace(-dataset.attrs['shape_diameter']//2, dataset.attrs['shape_diameter']//2, pts_per_side)\n",
    "    N = len(x)\n",
    "\n",
    "    arr_ew = np.reshape(dataset.element_weights.data, (N,N,dataset.element_weights.data.shape[1]))\n",
    "    da_ew = xr.DataArray(arr_ew, dims=[\"y\", \"x\", \"element_weight\"], coords=dict(y=y, x=x, element_weight=(\"element_weight\", [str(i) for i in dataset.elements.data[0]])))\n",
    "\n",
    "    arr_pw = np.reshape(dataset.phase_weights.data, (N,N,dataset.phase_weights.data.shape[1]))\n",
    "    da_phase = xr.DataArray(arr_pw, dims=[\"y\", \"x\", \"phase_weight\"], coords=dict(y=y, x=x, phase_weight=(\"phase_weight\", [str(i) for i in dataset.phase_names.data[0]])))\n",
    "\n",
    "    arr_iq = np.reshape(dataset.iq.data[:,1,:], (N,N,dataset.iq.data.shape[2]))\n",
    "    da_iq = xr.DataArray(arr_iq, dims=[\"y\", \"x\", \"intensity\"], coords=dict(y=y, x=x))\n",
    "\n",
    "    reshaped_ds = xr.Dataset({\n",
    "        'element_weights': da_ew,\n",
    "        'phase_weights': da_phase,\n",
    "        'iq': da_iq\n",
    "        })\n",
    "\n",
    "    # Copy in some existing attributes, add some new ones\n",
    "    reshaped_ds.attrs['description'] = dataset.attrs['description']\n",
    "    reshaped_ds.attrs['elements'] = [str(i) for i in dataset.elements.data[0]]\n",
    "    reshaped_ds.attrs['phases'] = [str(i) for i in dataset.phase_names.data[0]]\n",
    "    reshaped_ds.attrs['shape'] = dataset.attrs['shape']\n",
    "    reshaped_ds.attrs['shape_center'] = dataset.attrs['shape_center']\n",
    "    reshaped_ds.attrs['shape_width'] = dataset.attrs['shape_diameter']\n",
    "    reshaped_ds.attrs['resolution'] = dataset.attrs['resolution']\n",
    "    reshaped_ds.attrs['points_inside'] = dataset.attrs['points_inside']\n",
    "    reshaped_ds.attrs['Q'] = dataset.attrs['Q']\n",
    "\n",
    "    return reshaped_ds\n",
    "\n",
    "def save_ds(dataset: xr.Dataset, path: str, prefix: str, suffix: str, datetimestamp=True, remove_nc=False, drop_element_weights=True):\n",
    "\n",
    "    if drop_element_weights:\n",
    "    # for DRNets generated datasets\n",
    "        if 'element_weight' in dataset.coords:\n",
    "            dataset = dataset.drop_vars('element_weight')\n",
    "\n",
    "        if 'element_weights' in dataset.data_vars:\n",
    "            dataset = dataset.drop_vars('element_weights')\n",
    "\n",
    "    # file name setup\n",
    "    if datetimestamp:\n",
    "        datetimestr = datetime.now(tz=None).strftime(\"%d%b%Y_%H-%M-%S\") # time of saving ds, not creation (creation is multi-step so not timestamping)\n",
    "        file = f'{prefix}_{datetimestr}.nc'\n",
    "    else:\n",
    "        file = f'{prefix}.nc'\n",
    "\n",
    "    if suffix and type(suffix) == str:\n",
    "        file = file.replace('.nc', suffix + '.nc')\n",
    "\n",
    "    # check if file exists and throw an error if so\n",
    "    if os.path.isfile(os.path.join(path,file)):\n",
    "        raise NameError(f\"file already exists - edit prefix, suffix, or change argument 'datetimestamp' to True\")\n",
    "\n",
    "    #print(f'Dataset filename:\\n{file}')\n",
    "\n",
    "    dataset.to_netcdf(os.path.join(path, file))\n",
    "    with ZipFile(os.path.join(path,file.replace('.nc','.zip')), 'w', ZIP_DEFLATED) as zObject:\n",
    "        zObject.write(os.path.join(path,file), arcname=file)\n",
    "\n",
    "    if remove_nc:\n",
    "        # save space, keep GitHub happy by removing .nc files\n",
    "        os.remove(os.path.join(path,file))\n",
    "\n",
    "\n",
    "def ds_plot2D(dataset: xr.Dataset, dataarray: str, marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1, background_color='xkcd:eggshell', save=False):\n",
    "\n",
    "    if dataarray not in dataset.data_vars:\n",
    "        raise NameError(f'dataarray inputted ({dataarray}) is not a data variable in dataset inputted ({dataset})\\nAvailable data variables: {dataset.data_vars}')\n",
    "\n",
    "    if dataset[dataarray].shape[2] > 20:\n",
    "        raise ValueError(f'3rd dimension of array is larger than 10 - this would generate {dataset[dataarray].shape[2]} figures\\nThis is not the appropriate plotting tool for the inputted array')\n",
    "\n",
    "    for i in range(dataset[dataarray].shape[2]):\n",
    "        plt.figure()\n",
    "        if background_color:\n",
    "            ax = plt.axes()\n",
    "            ax.set_facecolor(background_color)\n",
    "        da_slice = dataset[dataarray][:, :, i]\n",
    "        da_slice.plot(cmap=cmap, vmin=vmin, vmax=vmax) # expects y = dim[0], x = dim[1]\n",
    "        if save:\n",
    "            if dataarray == 'phase_weights':\n",
    "                plt.savefig(f'{simwafer_path}/ds_{\"\".join(dataset.attrs['elements'])}_{dataset.attrs[\"phases\"][i].split('+')[1].split('_')[0]}_weight.png', dpi=300)\n",
    "            elif dataarray == 'element_weights':\n",
    "                plt.savefig(f'{simwafer_path}/ds_{\"\".join(dataset.attrs['elements'])}_{dataset.attrs[\"elements\"][i]}_weight.png', dpi=300)\n",
    "\n",
    "\n",
    "# simpler approach to setting element weights\n",
    "\n",
    "# num_compositions is 2, discrete_compositions should be len 2 (L / R of x_boundary)\n",
    "\n",
    "def set_elemental_comp_LR(dataset: xr.Dataset, num_compositions, elements_list, discrete_compositions, x_boundary='center'):\n",
    "    # some error checking\n",
    "    if num_compositions != discrete_compositions.shape[0]:\n",
    "        raise ValueError(f\"num_compositions ({num_compositions}) must be an integer matching discrete_compositions.shape[0] ({discrete_compositions.shape[0]})\")\n",
    "\n",
    "    if discrete_compositions.shape != (num_compositions, len(elements_list)):\n",
    "        raise ValueError(f\"discrete_compositions must have shape of (num_compositions, len(elements_list)) - currently {discrete_compositions.shape}\")\n",
    "\n",
    "    for num, sublist in enumerate(discrete_compositions):\n",
    "        if np.sum(sublist) != 1:\n",
    "            raise ValueError(f\"Sublist {num} in discrete_compositions does not sum to 1. Ensure sublist elements sum to 1 and try again.\\nSublist = {sublist}. Sum = {np.sum(sublist)}\")\n",
    "\n",
    "    # validate x_boundary\n",
    "    if x_boundary == 'center':\n",
    "        x_center, _ = dataset.attrs['shape_center']\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            if dataset.x.data.min() < x_boundary < dataset.x.data.max():\n",
    "                x_center = x_boundary\n",
    "            else:\n",
    "                raise ValueError(f'x_boundary {x_boundary} is out of bounds')\n",
    "        except TypeError:\n",
    "            raise TypeError(f\"x_boundary must be 'center' or a numeric value within x bounds - input is {x_boundary} (type={type(x_boundary)})\")\n",
    "\n",
    "    # setting weights based on boundary\n",
    "\n",
    "    # create an array of same shape as coords - iterate over\n",
    "    num_points = dataset.x.shape[0]\n",
    "\n",
    "    weights = np.zeros((num_points, len(elements_list)))\n",
    "\n",
    "    for i in range(num_points):\n",
    "        if dataset.x.data[i] < x_center:\n",
    "            weights[i] = discrete_compositions[0]\n",
    "        elif dataset.x.data[i] > x_center:\n",
    "            weights[i] = discrete_compositions[1]\n",
    "\n",
    "    # set weights to NaN where points are outside\n",
    "    if dataset.attrs['shape'] == 'circle':\n",
    "        diameter = dataset.attrs['shape_diameter']\n",
    "        center = dataset.attrs['shape_center'] # assuming always centering at origin\n",
    "\n",
    "        distances = np.sqrt((dataset.x.data - center[0])**2 + (dataset.y.data - center[1])**2)\n",
    "        mask = distances >= (diameter / 2)\n",
    "\n",
    "        weights[mask, :] = np.nan\n",
    "\n",
    "    # Assign coordinates and weights to the dataset\n",
    "    repeated_elements = np.tile(elements_list, (num_points, 1))\n",
    "\n",
    "    dataset = dataset.assign_coords(\n",
    "        elements=((\"points\", \"elements\"), repeated_elements),\n",
    "        element_weights=((\"points\", \"weights\"), weights)\n",
    "    )\n",
    "    #discrete_compositions_str = [str(comp) for comp in discrete_compositions]\n",
    "    #comp_points_str = [str(pos) for pos in comp_points]\n",
    "    #dataset.attrs['composition_centers (coords)'] = comp_points_str\n",
    "    #dataset.attrs['composition_centers (weights)'] = discrete_compositions_str\n",
    "    #dataset.attrs['smoothing_factor'] = smoothing_factor\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_symmetric_points_range(num_points, total_range):\n",
    "    half_points = num_points // 2\n",
    "\n",
    "    # Adjust the step size slightly if needed\n",
    "    step_size = total_range / half_points\n",
    "\n",
    "    if num_points % 2 == 0:\n",
    "        positive_points = np.linspace(step_size / 2, total_range - step_size / 2, half_points)\n",
    "        symmetric_points = np.concatenate((-positive_points[::-1], positive_points))\n",
    "    else:\n",
    "        positive_points = np.linspace(0, total_range, half_points + 1)\n",
    "        symmetric_points = np.concatenate((-positive_points[1:][::-1], positive_points))\n",
    "\n",
    "    if len(symmetric_points) != num_points:\n",
    "        raise ValueError(f'num_points = {num_points}, but len symmetric_points = {len(symmetric_points)}')\n",
    "\n",
    "    return symmetric_points\n",
    "\n",
    "def generate_symmetric_points_step(num_points, step_size):\n",
    "    half_points = num_points // 2\n",
    "    total_range = step_size * half_points\n",
    "\n",
    "    if num_points % 2 == 0:\n",
    "        positive_points = np.linspace(step_size / 2, total_range - step_size / 2, half_points)\n",
    "        symmetric_points = np.concatenate((-positive_points[::-1], positive_points))\n",
    "    else:\n",
    "        positive_points = np.linspace(0, total_range, half_points + 1)\n",
    "        symmetric_points = np.concatenate((-positive_points[1:][::-1], positive_points))\n",
    "\n",
    "    if len(symmetric_points) != num_points:\n",
    "        raise ValueError(f'num_points = {num_points}, but len symmetric_points = {len(symmetric_points)}')\n",
    "\n",
    "    return symmetric_points\n",
    "\n",
    "\n",
    "def ingest_data(filepath,\n",
    "                    dataset_description: str,\n",
    "                    dataset_shape: str,\n",
    "                    dataset_resolution: Union[float, int],\n",
    "                    dataset_Q_values: list,\n",
    "                    add_noise_percentage: Union[float, int]):\n",
    "\n",
    "    def generate_symmetric_points_step(num_points, step_size):\n",
    "        half_points = num_points // 2\n",
    "        total_range = step_size * half_points\n",
    "\n",
    "        if num_points % 2 == 0:\n",
    "            positive_points = np.linspace(step_size / 2, total_range - step_size / 2, half_points)\n",
    "            symmetric_points = np.concatenate((-positive_points[::-1], positive_points))\n",
    "        else:\n",
    "            positive_points = np.linspace(0, total_range, half_points + 1)\n",
    "            symmetric_points = np.concatenate((-positive_points[1:][::-1], positive_points))\n",
    "\n",
    "        if len(symmetric_points) != num_points:\n",
    "            raise ValueError(f'num_points = {num_points}, but len symmetric_points = {len(symmetric_points)}')\n",
    "\n",
    "        return symmetric_points\n",
    "\n",
    "    def add_noise_by_percentage(signal, noise_percentage):\n",
    "        \"\"\"\n",
    "        Add random noise to a NumPy array based on a percentage of the maximum signal value.\n",
    "\n",
    "        Parameters:\n",
    "            signal (numpy.ndarray): The original signal array.\n",
    "            noise_percentage (float): Percentage of the maximum signal value to use as noise.\n",
    "\n",
    "        Returns:\n",
    "            noisy_signal (numpy.ndarray): The signal array with added noise.\n",
    "        \"\"\"\n",
    "        # Find the maximum value in the signal\n",
    "        max_value = np.max(np.abs(signal))\n",
    "\n",
    "        # Calculate the noise standard deviation as a percentage of the max signal value\n",
    "        noise_std = (noise_percentage / 100) * max_value\n",
    "\n",
    "        # Generate random Gaussian noise with zero mean and calculated standard deviation\n",
    "        noise = np.random.normal(0, noise_std, signal.shape)\n",
    "\n",
    "        # Add the noise to the original signal\n",
    "        noisy_signal = signal + noise\n",
    "\n",
    "        return noisy_signal\n",
    "\n",
    "\n",
    "    # check that args are as expected\n",
    "    if dataset_shape not in ['circle','square']:\n",
    "        raise ValueError(f\"dataset_shape must be 'cirlce' or 'square' - input is {dataset_shape}\")\n",
    "\n",
    "    # read the file\n",
    "    data = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "    # assumes name, weights, and pattern are dict keys\n",
    "    names = [i['name'] for i in data]\n",
    "    weights = [i['weights'] for i in data]\n",
    "\n",
    "    # add noise if arg given\n",
    "    if add_noise_percentage:\n",
    "        patterns = [(add_noise_by_percentage(i['pattern'], add_noise_percentage)) for i in data]\n",
    "    else:\n",
    "        patterns = [i['pattern'] for i in data]\n",
    "\n",
    "    # Error checking:\n",
    "    # 1) shape of weight maps in weights must be symmetric\n",
    "    # 2) length of patterns must match the length of dataset_q_values\n",
    "    for i in range(data.shape[0]):\n",
    "        if weights[i].shape[0] != weights[i].shape[1]:\n",
    "            raise ValueError(f\"index {i} in weights has a non-symmetric shape - x points = {weights[i].shape[0]}, y points = {weights[i].shape[1]}\")\n",
    "\n",
    "        if len(patterns[i]) != len(dataset_Q_values):\n",
    "            raise ValueError(f\"index {i} in patterns has a length different from dataset_xaxis_values - Q len = {len(dataset_Q_values)}, patterns[{i}] len = {len(patterns[i])}\")\n",
    "\n",
    "    # some dataset assembly from inputs\n",
    "    pts_per_side = weights[0].shape[0] # number of points per side\n",
    "    x = generate_symmetric_points_step(pts_per_side, dataset_resolution)\n",
    "    y = generate_symmetric_points_step(pts_per_side, dataset_resolution)\n",
    "    N = len(x)\n",
    "\n",
    "    # make lists into arrays\n",
    "    weights_arr = np.array(weights)\n",
    "    patterns_arr = np.array(patterns)\n",
    "\n",
    "    weights_arr = weights_arr[:, ::-1, :]\n",
    "\n",
    "    if dataset_shape == 'circle':\n",
    "\n",
    "        if abs(y.max()) != abs(y.min()):\n",
    "            raise ValueError(f\"abs value of y.max and y.min should be equal but abs y.max = {abs(y.max())}, abs y.min = {abs(y.min())}\")\n",
    "\n",
    "        if abs(x.max()) != abs(x.min()):\n",
    "            raise ValueError(f\"abs value of x.max and x.min should be equal but abs x.max = {abs(x.max())}, abs x.min = {abs(x.min())}\")\n",
    "\n",
    "        if abs(x.max()) != abs(y.max()):\n",
    "            raise ValueError(f\"abs value of x.max and y.max should be equal but abs x.max = {abs(x.max())}, abs y.max = {abs(y.max())}\")\n",
    "\n",
    "        radius = y.max()\n",
    "\n",
    "        xv, yv = np.meshgrid(x, y) # create grid coordinates from 1D x,y arrays. xv, yv are 2D arays for each point on the grid\n",
    "        coords = np.column_stack([xv.ravel(), yv.ravel()])\n",
    "\n",
    "        center = (0,0)\n",
    "        distances = np.sqrt((coords[:,0] - center[0])**2 + (coords[:,1] - center[1])**2)\n",
    "        mask = distances >= (radius)\n",
    "        #points_inside = len([i for i in mask if not i])\n",
    "\n",
    "        mask_grid = np.reshape(mask, (len(x),len(x)))\n",
    "\n",
    "        for weight in weights_arr:\n",
    "            weight[mask_grid] = np.nan\n",
    "\n",
    "    # create iq_array\n",
    "    scaled_pattern_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        #pattern_map = weights_arr[i][np.newaxis, :, :] * patterns_arr[i]\n",
    "        pattern_map = weights_arr[i][np.newaxis, :, :] * patterns_arr[i][:, np.newaxis, np.newaxis]\n",
    "        scaled_pattern_list.append(pattern_map)\n",
    "\n",
    "    scaled_pattern_arr = np.array(scaled_pattern_list)\n",
    "    iq_arr = np.sum(scaled_pattern_arr, axis=0)\n",
    "\n",
    "    # prepare weights and iq for dataarray\n",
    "    #weights_arr_T = weights_arr.transpose(2, 1, 0)  # assumes shape of weights_arr is (weights, x, y) and we want (y, x, weights)4    weights_arr_T = weights_arr.transpose(2, 1, 0)  # assumes shape of weights_arr is (weights, x, y) and we want (y, x, weights)\n",
    "    weights_arr_T = weights_arr.transpose(1, 2, 0)  # assumes shape of weights_arr is (weights, x, y) and we want (y, x, weights)\n",
    "\n",
    "    if weights_arr_T.shape != (N, N, len(data)):\n",
    "        raise ValueError(f'weights array has a different shape than expected - expected = ({N, N, len(data)}), actual = ({weights_arr_T.shape})')\n",
    "\n",
    "    #iq_arr_T = iq_arr.transpose(2, 1, 0) # assumes shape of iq_arr is (iq, x, y) and we want (y, x, iq)\n",
    "    iq_arr_T = iq_arr.transpose(1, 2, 0) # assumes shape of iq_arr is (iq, x, y) and we want (y, x, iq)\n",
    "\n",
    "    if iq_arr_T.shape != (N, N, len(dataset_Q_values)):\n",
    "        raise ValueError(f'iq array has a different shape than expected - expected = ({N, N, len(dataset_Q_values)}), actual = ({iq_arr_T.shape})')\n",
    "\n",
    "    # Dataarray and dataset creation\n",
    "    da_phase = xr.DataArray(weights_arr_T, dims=[\"y\", \"x\", \"phase_weight\"], coords=dict(y=y, x=x, phase_weight=(\"phase_weight\", names)))\n",
    "    da_iq = xr.DataArray(iq_arr_T, dims=[\"y\", \"x\", \"intensity\"], coords=dict(y=y, x=x))\n",
    "\n",
    "    dataset = xr.Dataset({\n",
    "        'phase_weights': da_phase,\n",
    "        'iq': da_iq\n",
    "        })\n",
    "\n",
    "    # add attributes\n",
    "    dataset.attrs['description'] = dataset_description\n",
    "    dataset.attrs['phases'] = names\n",
    "    dataset.attrs['shape'] = dataset_shape\n",
    "    dataset.attrs['shape_center'] = (0,0)\n",
    "    dataset.attrs['shape_width'] = float(abs(x.max()) + (abs(x.min())))\n",
    "    dataset.attrs['resolution'] = dataset_resolution\n",
    "    dataset.attrs['Q'] = dataset_Q_values\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def add_noise_by_percentage(signal, noise_percentage):\n",
    "    \"\"\"\n",
    "    Add random noise to a NumPy array based on a percentage of the maximum signal value.\n",
    "\n",
    "    Parameters:\n",
    "        signal (numpy.ndarray): The original signal array.\n",
    "        noise_percentage (float): Percentage of the maximum signal value to use as noise.\n",
    "\n",
    "    Returns:\n",
    "        noisy_signal (numpy.ndarray): The signal array with added noise.\n",
    "    \"\"\"\n",
    "    # Find the maximum value in the signal\n",
    "    max_value = np.max(np.abs(signal))\n",
    "\n",
    "    # Calculate the noise standard deviation as a percentage of the max signal value\n",
    "    noise_std = (noise_percentage / 100) * max_value\n",
    "\n",
    "    # Generate random Gaussian noise with zero mean and calculated standard deviation\n",
    "    noise = np.random.normal(0, noise_std, signal.shape)\n",
    "\n",
    "    # Add the noise to the original signal\n",
    "    noisy_signal = signal + noise\n",
    "\n",
    "    return noisy_signal\n",
    "\n",
    "\n",
    "def add_validcoords_labels(reshaped_ds, weight_rounding=8, weight_cutoff=0.01, num_points_radius_reduced=3, short_legend_names=False):\n",
    "    if reshaped_ds.attrs['shape'] == 'circle':\n",
    "        x = reshaped_ds.x.data\n",
    "        y = reshaped_ds.y.data\n",
    "        xv, yv = np.meshgrid(x, y)\n",
    "        coords = np.column_stack([xv.ravel(), yv.ravel()])\n",
    "        distances = np.sqrt((coords[:,0] - reshaped_ds.attrs['shape_center'][0])**2 + (coords[:,1] - reshaped_ds.attrs['shape_center'][1])**2)\n",
    "        reduced_radius = reshaped_ds.attrs['shape_width'] / 2 - (reshaped_ds.attrs['resolution'] * num_points_radius_reduced)\n",
    "        mask = (distances >= reduced_radius)\n",
    "        coords_valid = np.array([coords[i] for i in range(coords.shape[0]) if mask[i] == False])\n",
    "\n",
    "    else:\n",
    "        x = reshaped_ds.x.data\n",
    "        y = reshaped_ds.y.data\n",
    "        xv, yv = np.meshgrid(x, y)\n",
    "        coords = np.column_stack([xv.ravel(), yv.ravel()])\n",
    "\n",
    "    # all indices (includes nans)\n",
    "    x_array = coords[:,0]\n",
    "    y_array = coords[:,1]\n",
    "    phase_weights = reshaped_ds.phase_weights.data.reshape(reshaped_ds.phase_weights.data.shape[0]*reshaped_ds.phase_weights.data.shape[1],reshaped_ds.phase_weights.data.shape[2])\n",
    "    phase_names = np.tile(reshaped_ds.phase_weight.data, (phase_weights.shape[0], 1))\n",
    "\n",
    "    nan_indices = [index for index, value in enumerate(phase_weights) if np.isnan(value).any()]\n",
    "\n",
    "    x_array = np.array([value for index, value in enumerate(x_array) if index not in nan_indices])\n",
    "    y_array = np.array([value for index, value in enumerate(y_array) if index not in nan_indices])\n",
    "    phase_weights = np.array([value for index, value in enumerate(phase_weights) if index not in nan_indices])\n",
    "    phase_names = np.array([value for index, value in enumerate(phase_names) if index not in nan_indices])\n",
    "    coords = np.column_stack((x_array,y_array)) # need to redefine this for further masking\n",
    "\n",
    "    # Create a mask for rows in `coords` that do not exist in `coords_valid`\n",
    "    mask = np.array([not np.any(np.all(coord == coords_valid, axis=1)) for coord in coords])\n",
    "\n",
    "    # Get the indices of rows in `coords` that are not in `coords_valid`\n",
    "    #missing_indices = np.where(mask)[0]\n",
    "\n",
    "    # Create a mask for the data array (True for the indices we want to keep)\n",
    "    mask_to_keep = ~mask\n",
    "\n",
    "    # Apply the mask to filter arrays\n",
    "    x_array = x_array[mask_to_keep]\n",
    "    y_array = y_array[mask_to_keep]\n",
    "    phase_weights = phase_weights[mask_to_keep]\n",
    "    phase_names = phase_names[mask_to_keep]\n",
    "    coords = np.column_stack((x_array, y_array)) # need to redefine this for use later\n",
    "\n",
    "    if short_legend_names:\n",
    "    # Generate phase_names_short array - only for DRnets names that are not shortened\n",
    "        phase_names_short = []\n",
    "        for names in phase_names:\n",
    "            #print(names)\n",
    "            phase_names_list = []\n",
    "            for name in names:\n",
    "                short_name = str(name).split('+')[1].split('_')[0]\n",
    "                phase_names_list.append(short_name)\n",
    "            phase_names_short.append(phase_names_list)\n",
    "\n",
    "        phase_names_short = np.array(phase_names_short)\n",
    "\n",
    "    # Generate phase_weights_rd array\n",
    "    phase_weights_rd = []\n",
    "    for weights in phase_weights:\n",
    "        phase_weights_list = []\n",
    "        for weight in weights:\n",
    "            weight_rd = round(weight,weight_rounding)\n",
    "            phase_weights_list.append(weight_rd)\n",
    "        phase_weights_rd.append(phase_weights_list)\n",
    "    phase_weights_rd = np.array(phase_weights_rd)\n",
    "\n",
    "    # get phases_present\n",
    "    phases_present = []\n",
    "\n",
    "    if short_legend_names:\n",
    "        for weights, names in zip(phase_weights_rd, phase_names_short):\n",
    "            phases_present_list = []\n",
    "            for weight, name in zip(weights,names):\n",
    "                if weight > weight_cutoff:\n",
    "                    phases_present_list.append(name)\n",
    "            phases_present.append(phases_present_list)\n",
    "    else:\n",
    "        for weights, names in zip(phase_weights_rd, phase_names):\n",
    "            phases_present_list = []\n",
    "            for weight, name in zip(weights,names):\n",
    "                if weight > weight_cutoff:\n",
    "                    phases_present_list.append(name)\n",
    "            phases_present.append(phases_present_list)\n",
    "\n",
    "    # Get the unique combinations of strings (order matters)\n",
    "    unique_combinations = []\n",
    "\n",
    "    # Collect unique combinations (as lists, preserving order)\n",
    "    for sublist in phases_present:\n",
    "        if sublist not in unique_combinations:\n",
    "            unique_combinations.append(sublist)\n",
    "\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    # Iterate over the main list and find matching unique combination index (order-sensitive)\n",
    "    for sublist in phases_present:\n",
    "        if sublist in unique_combinations:\n",
    "            ground_truth_labels.append(unique_combinations.index(sublist))\n",
    "        else:\n",
    "            raise ValueError(f'label mapping failed - sublist {sublist} not found in unique_combinations')\n",
    "\n",
    "    # Create DataArrays without coordinates\n",
    "    da_labels = xr.DataArray(ground_truth_labels, dims=['dim_1d'])\n",
    "    da_coords_valid = xr.DataArray(coords_valid, dims=['dim_x', 'dim_y'])\n",
    "\n",
    "    reshaped_ds['ground_truth_labels'] = da_labels\n",
    "    reshaped_ds['coords_valid'] = da_coords_valid\n",
    "\n",
    "    reshaped_ds.attrs['ground_truth_uniquecombs'] = [str(comb) for comb in unique_combinations] # can't store inhomogeneous shapes as attrs or vars\n",
    "    reshaped_ds.attrs['ground_truth_phasespresent'] = [str(phases) for phases in phases_present] # can't store inhomogeneous shapes as attrs or vars\n",
    "\n",
    "    return reshaped_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths - edit as needed (e.g., if repo root not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = get_repo_root()\n",
    "phasediagram_path = root_dir / \"Data\" / \"phasediagram_datasets\" # where you store Al-Li-Fe_dataset.nc\n",
    "simwafer_path = root_dir / \"Data\" / \"simulatedwafer_datasets\" # where you will save wafers you simulate\n",
    "print(f\"Root directory: {root_dir}\\nPhasediagram datasets path: {phasediagram_path}\\nSimulated wafer datasets path: {simwafer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Al-Li-Fe combinatorial library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading phase diagram dataset\n",
    "file = 'Al-Li-Fe_dataset.nc'\n",
    "ds_AlLiFe = xr.open_dataset(os.path.join(phasediagram_path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with coordinates\n",
    "testds = create_dataset_with_coords(shape='circle', diameter=60, resolution=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate elemental weights\n",
    "testds = calc_elemental_comps(dataset=testds, num_compositions=3, elements_list=['Al','Li','Fe'], discrete_compositions = np.array([[0.2,0.2,0.6], [0.2,0.6,0.2], [0.6,0.2,0.2]]), positions='calculated',\n",
    "                         calc_dist_scale = 100, deg_rotation=-90,\n",
    "                         discrete_comp_coords = None,\n",
    "                         find_on_grid=True,\n",
    "                         smoothing_factor=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate phase weights and I(Q), add to dataset - default adds noise!\n",
    "testds = interpolate_and_addtods(dataset=testds, dataset_DRNets=ds_AlLiFe, noise_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape ds onto meshgrid\n",
    "reshaped_ds = reshape_ds(testds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels, phasecombs, coords_valid needed in experiment\n",
    "reshaped_ds = add_validcoords_labels(reshaped_ds=reshaped_ds, weight_rounding=8, weight_cutoff=0.01, num_points_radius_reduced=3, short_legend_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "save_ds(dataset=reshaped_ds, path=simwafer_path, prefix='ds_AlLiFe_complex', suffix='', datetimestamp=True, remove_nc=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot element weights\n",
    "ds_plot2D(dataset=reshaped_ds, dataarray='element_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1, background_color='ivory') #background_color='xkcd:eggshell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Phase weights\n",
    "ds_plot2D(dataset=reshaped_ds, dataarray='phase_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1,background_color='ivory') #background_color='xkcd:eggshell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of summed diffraction pattern intensity\n",
    "summed_data = reshaped_ds.iq.sum(dim='intensity', keep_attrs=True)\n",
    "summed_data = summed_data.expand_dims(dim='intensity')\n",
    "plt.figure()\n",
    "summed_data.plot(cmap='bone_r')\n",
    "plt.title('Summed XRD intensity on wafer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating 3 libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al-Li-Fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading phase diagram dataset\n",
    "#phasediagram_path = 'C:\\\\GitHub\\\\ExperimentSimulator\\\\Data\\\\phasediagram_datasets' # now defined at top of notebook\n",
    "file = 'Al-Li-Fe_dataset.nc'\n",
    "ds_AlLiFe = xr.open_dataset(os.path.join(phasediagram_path,file))\n",
    "# Create dataset with coordinates\n",
    "testds = create_dataset_with_coords(shape='circle', diameter=60, resolution=0.4)\n",
    "# Calculate elemental weights\n",
    "testds = calc_elemental_comps(dataset=testds, num_compositions=3, elements_list=['Al','Li','Fe'], discrete_compositions = np.array([[0.2,0.2,0.6], [0.2,0.6,0.2], [0.6,0.2,0.2]]), positions='calculated',\n",
    "                         calc_dist_scale = 100, deg_rotation=-90,\n",
    "                         discrete_comp_coords = None,\n",
    "                         find_on_grid=True,\n",
    "                         smoothing_factor=5.0)\n",
    "# Interpolate phase weights and I(Q), add to dataset - default adds noise!\n",
    "testds = interpolate_and_addtods(dataset=testds, dataset_DRNets=ds_AlLiFe, noise_percentage=0.01)\n",
    "# Reshape ds onto meshgrid\n",
    "reshaped_ds = reshape_ds(testds)\n",
    "# get labels, phasecombs, coords_valid needed in experiment\n",
    "reshaped_ds = add_validcoords_labels(reshaped_ds=reshaped_ds, weight_rounding=8, weight_cutoff=0.01, num_points_radius_reduced=3, short_legend_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset -this breaks if the file already exists and is open - possibly because of how the file contents are interacted with on disk\n",
    "save_ds(dataset=reshaped_ds, path=simwafer_path, prefix='ds_AlLiFe_complex', suffix='', datetimestamp=True, remove_nc=False, drop_element_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plot2D(dataset=reshaped_ds, dataarray='element_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1, background_color='ivory', save=True) #background_color='xkcd:eggshell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plot2D(dataset=reshaped_ds, dataarray='phase_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1,background_color='ivory', save=True) #background_color='xkcd:eggshell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Cu-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading phase diagram dataset\n",
    "#phasediagram_path = 'C:\\\\GitHub\\\\ExperimentSimulator\\\\Data\\\\phasediagram_datasets' # now defined at top of notebook\n",
    "file = 'Bi-Cu-V_dataset.nc'\n",
    "ds_BiCuV = xr.open_dataset(os.path.join(phasediagram_path,file))\n",
    "# Create dataset with coordinates\n",
    "testds = create_dataset_with_coords(shape='circle', diameter=60, resolution=0.4)\n",
    "# Calculate elemental weights\n",
    "testds = calc_elemental_comps(dataset=testds, num_compositions=3, elements_list=['Bi','Cu','V'], discrete_compositions = np.array([[0.2,0.2,0.6], [0.2,0.6,0.2], [0.6,0.2,0.2]]), positions='calculated',\n",
    "                         calc_dist_scale = 100, deg_rotation=-90,\n",
    "                         discrete_comp_coords = None,\n",
    "                         find_on_grid=True,\n",
    "                         smoothing_factor=5.0)\n",
    "# Interpolate phase weights and I(Q), add to dataset - default adds noise!\n",
    "testds = interpolate_and_addtods(dataset=testds, dataset_DRNets=ds_BiCuV, noise_percentage=0.01)\n",
    "# Reshape ds onto meshgrid\n",
    "reshaped_ds = reshape_ds(testds)\n",
    "# get labels, phasecombs, coords_valid needed in experiment\n",
    "reshaped_ds = add_validcoords_labels(reshaped_ds=reshaped_ds, weight_rounding=8, weight_cutoff=0.01, num_points_radius_reduced=3, short_legend_names=True)\n",
    "\n",
    "# Save dataset -this breaks if the file already exists and is open - possibly because of how the file contents are interacted with on disk\n",
    "save_ds(dataset=reshaped_ds, path=simwafer_path, prefix='ds_BiCuV_complex', suffix='', datetimestamp=True, remove_nc=False, drop_element_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plot2D(dataset=reshaped_ds, dataarray='element_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1, background_color='ivory', save=True) #background_color='xkcd:eggshell')\n",
    "ds_plot2D(dataset=reshaped_ds, dataarray='phase_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1,background_color='ivory', save=True) #background_color='xkcd:eggshell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Li-Sr-Al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading phase diagram dataset\n",
    "#phasediagram_path = 'C:\\\\GitHub\\\\ExperimentSimulator\\\\Data\\\\phasediagram_datasets' # now defined at top of notebook\n",
    "file = 'Li-Sr-Al_dataset.nc'\n",
    "ds_LiSrAl = xr.open_dataset(os.path.join(phasediagram_path,file))\n",
    "# Create dataset with coordinates\n",
    "testds = create_dataset_with_coords(shape='circle', diameter=60, resolution=0.4)\n",
    "# Calculate elemental weights\n",
    "testds = calc_elemental_comps(dataset=testds, num_compositions=3, elements_list=['Li','Sr','Al'], discrete_compositions = np.array([[0.2,0.2,0.6], [0.2,0.6,0.2], [0.6,0.2,0.2]]), positions='calculated',\n",
    "                         calc_dist_scale = 100, deg_rotation=-90,\n",
    "                         discrete_comp_coords = None,\n",
    "                         find_on_grid=True,\n",
    "                         smoothing_factor=5.0)\n",
    "# Interpolate phase weights and I(Q), add to dataset - default adds noise!\n",
    "testds = interpolate_and_addtods(dataset=testds, dataset_DRNets=ds_LiSrAl, noise_percentage=0.01)\n",
    "# Reshape ds onto meshgrid\n",
    "reshaped_ds = reshape_ds(testds)\n",
    "# get labels, phasecombs, coords_valid needed in experiment\n",
    "reshaped_ds = add_validcoords_labels(reshaped_ds=reshaped_ds, weight_rounding=8, weight_cutoff=0.01, num_points_radius_reduced=3, short_legend_names=True)\n",
    "\n",
    "# Save dataset -this breaks if the file already exists and is open - possibly because of how the file contents are interacted with on disk\n",
    "save_ds(dataset=reshaped_ds, path=simwafer_path, prefix='ds_LiSrAl_complex', suffix='', datetimestamp=True, remove_nc=False, drop_element_weights=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plot2D(dataset=reshaped_ds, dataarray='element_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1, background_color='ivory', save=True) #background_color='xkcd:eggshell')\n",
    "ds_plot2D(dataset=reshaped_ds, dataarray='phase_weights', marker='s', marker_size=2, cmap='bone_r',vmin=0,vmax=1,background_color='ivory', save=True) #background_color='xkcd:eggshell')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
